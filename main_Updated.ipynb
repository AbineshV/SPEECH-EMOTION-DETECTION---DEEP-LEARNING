{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RysTVgUDs0Q",
        "outputId": "16a93333-d75d-4709-d3dc-c6c2f52be1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set dataset folder path (update if needed)\n",
        "AUDIO_FOLDER = '/content/drive/My Drive/audio/audio'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GDDdbMtWFRmw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "SAMPLE_RATE = 16000\n",
        "SEGMENT_LENGTH = 3  # Segment length in seconds\n",
        "ALEXNET_INPUT_SIZE = (227, 227)\n",
        "\n",
        "# EMO-DB Emotion Mapping\n",
        "EMOTION_MAP = {\n",
        "    'W': 'anger',\n",
        "    'L': 'boredom',\n",
        "    'A': 'anxiety',\n",
        "    'F': 'happiness',\n",
        "    'T': 'sadness',\n",
        "    'E': 'disgust',\n",
        "    'N': 'neutral'\n",
        "}\n"
      ],
      "metadata": {
        "id": "zRmenKNYGwU0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained AlexNet (Truncated)\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "alexnet.classifier = alexnet.classifier[:6]  # Remove final classification layer\n",
        "alexnet.eval()\n",
        "\n",
        "# Image transformation for AlexNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRgHEZZfGxRO",
        "outputId": "e45cb961-f450-44bd-d0d4-ced0067b3570"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_log_mel_spectrogram(audio_file, segment_length=3):\n",
        "    y, sr = librosa.load(audio_file, sr=SAMPLE_RATE)\n",
        "    segment_samples = segment_length * sr\n",
        "    segments = []\n",
        "\n",
        "    for i in range(0, len(y) - segment_samples, segment_samples // 2):\n",
        "        segment = y[i:i+segment_samples]\n",
        "        mel_spec = librosa.feature.melspectrogram(y=segment, sr=sr, n_mels=64)\n",
        "        log_mel = librosa.power_to_db(mel_spec)\n",
        "\n",
        "        delta = librosa.feature.delta(log_mel)\n",
        "        delta_delta = librosa.feature.delta(log_mel, order=2)\n",
        "\n",
        "        combined = np.stack([log_mel, delta, delta_delta], axis=-1)\n",
        "        resized = cv2.resize(combined, ALEXNET_INPUT_SIZE)\n",
        "\n",
        "        segments.append(resized)\n",
        "\n",
        "    return segments\n"
      ],
      "metadata": {
        "id": "nOvhwu38Gzpi"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_segments(segments):\n",
        "    features = []\n",
        "    for segment in segments:\n",
        "        tensor = transform(segment).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            feature = alexnet(tensor).squeeze().numpy()\n",
        "        features.append(feature)\n",
        "    return np.array(features)\n"
      ],
      "metadata": {
        "id": "0B9i3-JlG15H"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dtpm_pooling(segment_features):\n",
        "    levels = [1, 2, 4]\n",
        "    pooled_features = []\n",
        "\n",
        "    for level in levels:\n",
        "        split_size = len(segment_features) // level\n",
        "        for i in range(level):\n",
        "            start = i * split_size\n",
        "            end = (i+1) * split_size if i != level-1 else len(segment_features)\n",
        "            segment_part = segment_features[start:end]\n",
        "\n",
        "            pooled_part = np.mean(segment_part, axis=0) if len(segment_part) > 0 else np.zeros(segment_features.shape[1])\n",
        "            pooled_features.append(pooled_part)\n",
        "\n",
        "    return np.concatenate(pooled_features)\n"
      ],
      "metadata": {
        "id": "7rxMzgFmG4OG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_filename(filename):\n",
        "    speaker = filename[:2]\n",
        "    emotion = EMOTION_MAP.get(filename[5], None)\n",
        "    return speaker, emotion\n"
      ],
      "metadata": {
        "id": "61q489JMG6_p"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert emotion labels to numerical indices\n",
        "EMOTION_TO_INDEX = {emotion: idx for idx, emotion in enumerate(EMOTION_MAP.values())}\n",
        "\n",
        "def prepare_dataset():\n",
        "    data, labels, speakers = [], [], []\n",
        "\n",
        "    for file in os.listdir(AUDIO_FOLDER):\n",
        "        if not file.endswith('.wav'):\n",
        "            continue\n",
        "\n",
        "        speaker, emotion = parse_filename(file)\n",
        "        if not emotion:\n",
        "            continue\n",
        "\n",
        "        segments = extract_log_mel_spectrogram(os.path.join(AUDIO_FOLDER, file))\n",
        "        if not segments:\n",
        "            continue\n",
        "\n",
        "        features = extract_features_from_segments(segments)\n",
        "        pooled_features = dtpm_pooling(features)\n",
        "\n",
        "        data.append(pooled_features)\n",
        "        labels.append(EMOTION_TO_INDEX[emotion])  # Convert emotion to index\n",
        "        speakers.append(speaker)\n",
        "\n",
        "    return np.array(data), np.array(labels, dtype=np.int64), np.array(speakers)\n"
      ],
      "metadata": {
        "id": "hOdD2L0yG9do"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loso_cross_validation(features, labels, speakers):\n",
        "    unique_speakers = np.unique(speakers)\n",
        "    all_predictions, all_true_labels = [], []\n",
        "\n",
        "    for test_speaker in unique_speakers:\n",
        "        train_indices = np.where(speakers != test_speaker)[0]\n",
        "        test_indices = np.where(speakers == test_speaker)[0]\n",
        "\n",
        "        if len(test_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train, y_train = features[train_indices], labels[train_indices]\n",
        "        X_test, y_test = features[test_indices], labels[test_indices]\n",
        "\n",
        "        clf = SVC(kernel='linear')\n",
        "        clf.fit(X_train, y_train)\n",
        "        predictions = clf.predict(X_test)\n",
        "\n",
        "        all_predictions.extend(predictions)\n",
        "        all_true_labels.extend(y_test)\n",
        "\n",
        "        print(f'LOSO - Leaving out Speaker {test_speaker}: Accuracy = {np.mean(predictions == y_test):.2f}')\n",
        "\n",
        "    print(\"\\nFinal Classification Report (LOSO Cross-Validation):\")\n",
        "    print(classification_report(all_true_labels, all_predictions, target_names=list(EMOTION_MAP.values())))\n"
      ],
      "metadata": {
        "id": "br3YbK2VG_eK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Preparing dataset...\")\n",
        "    features, labels, speakers = prepare_dataset()\n",
        "    #print(f\"Extracted {len(features)} samples from {len(np.unique(speakers))} speakers.\")\n",
        "\n",
        "    print(\"Running LOSO Cross-Validation...\")\n",
        "    loso_cross_validation(features, labels, speakers)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFgcCx7cHEHS",
        "outputId": "4bd9e572-1115-46a8-c489-527db406e778"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset...\n",
            "Running LOSO Cross-Validation...\n",
            "LOSO - Leaving out Speaker 03: Accuracy = 0.44\n",
            "LOSO - Leaving out Speaker 08: Accuracy = 0.69\n",
            "LOSO - Leaving out Speaker 09: Accuracy = 0.41\n",
            "LOSO - Leaving out Speaker 10: Accuracy = 0.57\n",
            "LOSO - Leaving out Speaker 11: Accuracy = 0.45\n",
            "LOSO - Leaving out Speaker 12: Accuracy = 0.69\n",
            "LOSO - Leaving out Speaker 13: Accuracy = 0.33\n",
            "LOSO - Leaving out Speaker 14: Accuracy = 0.60\n",
            "LOSO - Leaving out Speaker 15: Accuracy = 0.67\n",
            "LOSO - Leaving out Speaker 16: Accuracy = 0.50\n",
            "\n",
            "Final Classification Report (LOSO Cross-Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.49      0.60      0.54        35\n",
            "     boredom       0.49      0.55      0.51        33\n",
            "     anxiety       0.00      0.00      0.00         9\n",
            "   happiness       0.21      0.15      0.18        20\n",
            "     sadness       0.80      0.83      0.81        47\n",
            "     disgust       0.47      0.36      0.41        25\n",
            "     neutral       0.50      0.56      0.53        16\n",
            "\n",
            "    accuracy                           0.54       185\n",
            "   macro avg       0.42      0.44      0.43       185\n",
            "weighted avg       0.51      0.54      0.52       185\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "lsqLmZcrpWoc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define BiLSTM Model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # BiLSTM outputs hidden_dim * 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Use last time step\n",
        "        return out"
      ],
      "metadata": {
        "id": "dXalshzGpXts"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataloader(features, labels, batch_size=16):\n",
        "    # Add sequence length dimension (batch, seq_len=1, feat_dim)\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(1)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    dataset = TensorDataset(features_tensor, labels_tensor)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "nEvofESPpZuX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train_bilstm(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "r3AbrEZjpb22"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Workflow\n",
        "features, labels, speakers = prepare_dataset()\n",
        "dataloader = prepare_dataloader(features, labels)\n",
        "\n",
        "input_dim = features.shape[1]\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "output_dim = len(EMOTION_MAP)\n",
        "\n",
        "bilstm_model = BiLSTM(input_dim, hidden_dim, num_layers, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(bilstm_model.parameters(), lr=0.001)\n",
        "\n",
        "train_bilstm(bilstm_model, dataloader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NGlLloFpeNQ",
        "outputId": "7d646b0a-4c55-4bef-97bf-e472f6ebe872"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.9061\n",
            "Epoch 2, Loss: 1.8096\n",
            "Epoch 3, Loss: 1.6988\n",
            "Epoch 4, Loss: 1.6321\n",
            "Epoch 5, Loss: 1.5526\n",
            "Epoch 6, Loss: 1.5104\n",
            "Epoch 7, Loss: 1.4957\n",
            "Epoch 8, Loss: 1.4746\n",
            "Epoch 9, Loss: 1.4479\n",
            "Epoch 10, Loss: 1.4886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bilstm(model, features, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Ensure features are shaped correctly (batch_size, seq_len=1, feature_dim)\n",
        "        inputs = torch.tensor(features, dtype=torch.float32).unsqueeze(1)\n",
        "        targets = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        predictions = torch.argmax(outputs, dim=1).numpy()\n",
        "        true_labels = targets.numpy()\n",
        "\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=list(EMOTION_MAP.values()), zero_division=0)\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    return acc, report\n"
      ],
      "metadata": {
        "id": "kr-HcA1or6Fe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loso_bilstm(features, labels, speakers):\n",
        "    unique_speakers = np.unique(speakers)\n",
        "    all_predictions, all_true_labels = [], []\n",
        "\n",
        "    for test_speaker in unique_speakers:\n",
        "        train_indices = np.where(speakers != test_speaker)[0]\n",
        "        test_indices = np.where(speakers == test_speaker)[0]\n",
        "\n",
        "        if len(test_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train, y_train = features[train_indices], labels[train_indices]\n",
        "        X_test, y_test = features[test_indices], labels[test_indices]\n",
        "\n",
        "        train_loader = prepare_dataloader(X_train, y_train)\n",
        "\n",
        "        bilstm_model = BiLSTM(X_train.shape[1], 128, 2, len(EMOTION_MAP))\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(bilstm_model.parameters(), lr=0.001)\n",
        "\n",
        "        train_bilstm(bilstm_model, train_loader, criterion, optimizer)\n",
        "\n",
        "        # Ensure X_test is reshaped correctly before passing to the model\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
        "        predictions = torch.argmax(bilstm_model(X_test_tensor), dim=1).numpy()\n",
        "\n",
        "        all_predictions.extend(predictions)\n",
        "        all_true_labels.extend(y_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, predictions)\n",
        "        print(f'LOSO - Leaving out Speaker {test_speaker}: Accuracy = {acc:.2f}')\n",
        "\n",
        "    print(\"\\nFinal Classification Report (LOSO Cross-Validation):\")\n",
        "    print(classification_report(all_true_labels, all_predictions, target_names=list(EMOTION_MAP.values()), zero_division=0))\n"
      ],
      "metadata": {
        "id": "2cSJ1gD4r8n-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bilstm(bilstm_model, features, labels)\n",
        "loso_bilstm(features, labels, speakers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16YundIwrp91",
        "outputId": "9c5d1aa5-c36d-4fca-9eb7-b7a88354528e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5027\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.60      0.60      0.60        35\n",
            "     boredom       0.35      0.82      0.49        33\n",
            "     anxiety       0.00      0.00      0.00         9\n",
            "   happiness       0.00      0.00      0.00        20\n",
            "     sadness       0.63      0.94      0.75        47\n",
            "     disgust       0.50      0.04      0.07        25\n",
            "     neutral       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.50       185\n",
            "   macro avg       0.30      0.34      0.27       185\n",
            "weighted avg       0.40      0.50      0.40       185\n",
            "\n",
            "Epoch 1, Loss: 1.9078\n",
            "Epoch 2, Loss: 1.7767\n",
            "Epoch 3, Loss: 1.6978\n",
            "Epoch 4, Loss: 1.5856\n",
            "Epoch 5, Loss: 1.5027\n",
            "Epoch 6, Loss: 1.5442\n",
            "Epoch 7, Loss: 1.4963\n",
            "Epoch 8, Loss: 1.5267\n",
            "Epoch 9, Loss: 1.4876\n",
            "Epoch 10, Loss: 1.3915\n",
            "LOSO - Leaving out Speaker 03: Accuracy = 0.38\n",
            "Epoch 1, Loss: 1.9088\n",
            "Epoch 2, Loss: 1.8241\n",
            "Epoch 3, Loss: 1.7827\n",
            "Epoch 4, Loss: 1.7229\n",
            "Epoch 5, Loss: 1.6591\n",
            "Epoch 6, Loss: 1.6180\n",
            "Epoch 7, Loss: 1.5900\n",
            "Epoch 8, Loss: 1.5640\n",
            "Epoch 9, Loss: 1.4835\n",
            "Epoch 10, Loss: 1.4955\n",
            "LOSO - Leaving out Speaker 08: Accuracy = 0.62\n",
            "Epoch 1, Loss: 1.8752\n",
            "Epoch 2, Loss: 1.7893\n",
            "Epoch 3, Loss: 1.7099\n",
            "Epoch 4, Loss: 1.6266\n",
            "Epoch 5, Loss: 1.5640\n",
            "Epoch 6, Loss: 1.5014\n",
            "Epoch 7, Loss: 1.5497\n",
            "Epoch 8, Loss: 1.5233\n",
            "Epoch 9, Loss: 1.5420\n",
            "Epoch 10, Loss: 1.5553\n",
            "LOSO - Leaving out Speaker 09: Accuracy = 0.18\n",
            "Epoch 1, Loss: 1.9088\n",
            "Epoch 2, Loss: 1.8185\n",
            "Epoch 3, Loss: 1.6701\n",
            "Epoch 4, Loss: 1.6321\n",
            "Epoch 5, Loss: 1.6370\n",
            "Epoch 6, Loss: 1.5795\n",
            "Epoch 7, Loss: 1.5078\n",
            "Epoch 8, Loss: 1.4537\n",
            "Epoch 9, Loss: 1.3906\n",
            "Epoch 10, Loss: 1.4696\n",
            "LOSO - Leaving out Speaker 10: Accuracy = 0.71\n",
            "Epoch 1, Loss: 1.8820\n",
            "Epoch 2, Loss: 1.8297\n",
            "Epoch 3, Loss: 1.6832\n",
            "Epoch 4, Loss: 1.6318\n",
            "Epoch 5, Loss: 1.5059\n",
            "Epoch 6, Loss: 1.5178\n",
            "Epoch 7, Loss: 1.3938\n",
            "Epoch 8, Loss: 1.3859\n",
            "Epoch 9, Loss: 1.4804\n",
            "Epoch 10, Loss: 1.3873\n",
            "LOSO - Leaving out Speaker 11: Accuracy = 0.36\n",
            "Epoch 1, Loss: 1.8918\n",
            "Epoch 2, Loss: 1.8206\n",
            "Epoch 3, Loss: 1.7549\n",
            "Epoch 4, Loss: 1.6943\n",
            "Epoch 5, Loss: 1.6323\n",
            "Epoch 6, Loss: 1.5632\n",
            "Epoch 7, Loss: 1.5053\n",
            "Epoch 8, Loss: 1.5060\n",
            "Epoch 9, Loss: 1.4992\n",
            "Epoch 10, Loss: 1.4881\n",
            "LOSO - Leaving out Speaker 12: Accuracy = 0.54\n",
            "Epoch 1, Loss: 1.8800\n",
            "Epoch 2, Loss: 1.7762\n",
            "Epoch 3, Loss: 1.6954\n",
            "Epoch 4, Loss: 1.6200\n",
            "Epoch 5, Loss: 1.5483\n",
            "Epoch 6, Loss: 1.4772\n",
            "Epoch 7, Loss: 1.5316\n",
            "Epoch 8, Loss: 1.4770\n",
            "Epoch 9, Loss: 1.4818\n",
            "Epoch 10, Loss: 1.5037\n",
            "LOSO - Leaving out Speaker 13: Accuracy = 0.33\n",
            "Epoch 1, Loss: 1.9106\n",
            "Epoch 2, Loss: 1.8183\n",
            "Epoch 3, Loss: 1.7537\n",
            "Epoch 4, Loss: 1.6846\n",
            "Epoch 5, Loss: 1.6491\n",
            "Epoch 6, Loss: 1.5763\n",
            "Epoch 7, Loss: 1.5923\n",
            "Epoch 8, Loss: 1.5150\n",
            "Epoch 9, Loss: 1.4789\n",
            "Epoch 10, Loss: 1.4750\n",
            "LOSO - Leaving out Speaker 14: Accuracy = 0.40\n",
            "Epoch 1, Loss: 1.8801\n",
            "Epoch 2, Loss: 1.7932\n",
            "Epoch 3, Loss: 1.7020\n",
            "Epoch 4, Loss: 1.6162\n",
            "Epoch 5, Loss: 1.5171\n",
            "Epoch 6, Loss: 1.4531\n",
            "Epoch 7, Loss: 1.4574\n",
            "Epoch 8, Loss: 1.3947\n",
            "Epoch 9, Loss: 1.3440\n",
            "Epoch 10, Loss: 1.3740\n",
            "LOSO - Leaving out Speaker 15: Accuracy = 0.58\n",
            "Epoch 1, Loss: 1.8894\n",
            "Epoch 2, Loss: 1.7799\n",
            "Epoch 3, Loss: 1.6744\n",
            "Epoch 4, Loss: 1.6026\n",
            "Epoch 5, Loss: 1.5796\n",
            "Epoch 6, Loss: 1.5372\n",
            "Epoch 7, Loss: 1.5271\n",
            "Epoch 8, Loss: 1.5390\n",
            "Epoch 9, Loss: 1.5286\n",
            "Epoch 10, Loss: 1.4229\n",
            "LOSO - Leaving out Speaker 16: Accuracy = 0.41\n",
            "\n",
            "Final Classification Report (LOSO Cross-Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.42      0.77      0.55        35\n",
            "     boredom       0.25      0.39      0.30        33\n",
            "     anxiety       0.00      0.00      0.00         9\n",
            "   happiness       0.00      0.00      0.00        20\n",
            "     sadness       0.60      0.85      0.70        47\n",
            "     disgust       0.00      0.00      0.00        25\n",
            "     neutral       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.43       185\n",
            "   macro avg       0.18      0.29      0.22       185\n",
            "weighted avg       0.28      0.43      0.34       185\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}